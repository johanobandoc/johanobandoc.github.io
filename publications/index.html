<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>publications | Johan Obando-Ceron</title>
  <meta name="author" content="Johan Obando-Ceron">
  <meta name="description" content="publications by categories in reversed chronological order.">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css"
    integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">
  <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css"
    integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css"
    integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
  <link rel="stylesheet" type="text/css"
    href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media=""
    id="highlight_theme_light">
  <link rel="shortcut icon"
    href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
  <link rel="stylesheet" href="../assets/css/main.css">
  <link rel="canonical" href="index.html">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none"
    id="highlight_theme_dark">
  <script src="../assets/js/theme.js"></script>
  <script src="../assets/js/dark_mode.js"></script>
</head>

<body class="fixed-top-nav sticky-bottom-footer">
  <header>
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
      <div class="container"> <a class="navbar-brand title font-weight-lighter" href="../index.html">Johan Obando-Ceron</a>
        <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
          aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle
            navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span
            class="icon-bar bottom-bar"></span> </button>
        <div class="collapse navbar-collapse text-right" id="navbarNav">
          <ul class="navbar-nav ml-auto flex-nowrap">
            <li class="nav-item "> <a class="nav-link" href="../index.html">about</a> </li>
            <li class="nav-item active"> <a class="nav-link" href="index.html">publications<span
                  class="sr-only">(current)</span></a> </li>
            <li class="nav-item "> <a class="nav-link" href="../cv/index.html">cv</a> </li>
            <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i
                  class="fas fa-sun"></i> </button> </li>
          </ul>
        </div>
      </div>
    </nav> <progress id="progress" value="0">
      <div class="progress-container"> <span class="progress-bar"></span> </div>
    </progress>
  </header>
  <div class="container mt-5">
    <div class="post">
      <header class="post-header">
        <h1 class="post-title">Publications</h1>
        <p class="post-description">Publications by categories in reversed chronological order.</p>
      </header>
      <article>
        <div class="publications">
          <h2 class="bibliography">2024</h2>
          <ol class="bibliography">
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#9d02d7"><a
                      href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr></div>
                <div id="sokar2024dontflattentokenizeunlocking" class="col-sm-8">
                  <div class="title">Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL</div>
                  <div class="author">Gada Sokar, <em>Johan Obando-Ceron</em>, Hugo Larochelle, Aaron Courville and Pablo Samuel Castro</div>
                  <div class="periodical"> <em>In submission</em>, 2024 </div>
                  <div class="periodical"> </div>
                  <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a
                      href="https://arxiv.org/pdf/2410.01930" class="btn btn-sm z-depth-0" role="button"
                      rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0"
                      role="button">Bib</a>  
                  </div>
                  <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true"
                      data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span
                      class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true"
                      data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div>
                  <div class="abstract hidden">
                    <p>The use of deep neural networks in reinforcement learning (RL) often suffers from performance 
                      degradation as model size increases. While soft mixtures of experts (SoftMoEs) have recently shown 
                      promise in mitigating this issue for online RL, the reasons behind their effectiveness remain 
                      largely unknown. In this work we provide an in-depth analysis identifying the key factors driving 
                      this performance gain. We discover the surprising result that tokenizing the encoder output, rather 
                      than the use of multiple experts, is what is behind the efficacy of SoftMoEs. Indeed, we demonstrate
                       that even with an appropriately scaled single expert, we are able to maintain the performance gains, 
                       largely thanks to tokenization.</p>
                  </div>
                  <div class="bibtex hidden">
                    <figure class="highlight">
                      <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sokar2024dontflattentokenizeunlocking</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghada Sokar and Johan Obando-Ceron and Aaron Courville and Hugo Larochelle and Pablo Samuel Castro}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2410.01930}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre>
                    </figure>
                  </div>
                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#9d02d7"><a
                      href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr></div>
                <div id="liu2024neuroplasticexpansiondeepreinforcement" class="col-sm-8">
                  <div class="title">Neuroplastic Expansion in Deep Reinforcement Learning</div>
                  <div class="author"> Jiashun Liu, <em>Johan Obando-Ceron</em>, Aaron Courville and Ling Pan</div>
                  <div class="periodical"> <em>In submission</em>, 2024 </div>
                  <div class="periodical"> </div>
                  <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a
                      href="https://arxiv.org/pdf/2410.07994" class="btn btn-sm z-depth-0" role="button"
                      rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0"
                      role="button">Bib</a> </div>
                  <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true"
                      data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span
                      class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true"
                      data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div>
                  <div class="abstract hidden">
                    <p>The loss of plasticity in learning agents, analogous to the solidification of neural pathways in biological brains, 
                      significantly impedes learning and adaptation in reinforcement learning due to its non-stationary nature. To address 
                      this fundamental challenge, we propose a novel approach, Neuroplastic Expansion (NE), inspired by cortical expansion 
                      in cognitive science. NE maintains learnability and adaptability throughout the entire training process by dynamically 
                      growing the network from a smaller initial size to its full dimension. Our method is designed with three key 
                      components: (1) elastic neuron generation based on potential gradients, (2) dormant neuron pruning to optimize network 
                      expressivity, and (3) neuron consolidation via experience review to strike a balance in the plasticity-stability 
                      dilemma. Extensive experiments demonstrate that NE effectively mitigates plasticity loss and outperforms 
                      state-of-the-art methods across various tasks in MuJoCo and DeepMind Control Suite environments. NE enables more 
                      adaptive learning in complex, dynamic environments, which represents a crucial step towards transitioning deep 
                      reinforcement learning from static, one-time training paradigms to more flexible, continually adapting models.</p>
                  </div>
                  <div class="bibtex hidden">
                    <figure class="highlight">ßß
                      <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2024neuroplasticexpansiondeepreinforcement</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neuroplastic Expansion in Deep Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiashun Liu and Johan Obando-Ceron and Aaron Courville and Ling Pan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2410.07994}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre>
                    </figure>
                  </div>
                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#ffb14e"><a
                      href="https://rl-conference.cc" rel="external nofollow noopener" target="_blank">RLC</a></abbr></div>
                <div id="willi2024mixtureexpertsmixturerl" class="col-sm-8">
                  <div class="title">Mixture of Experts in a Mixture of RL settings</div>
                  <div class="author"> Timon Willi*, <em>Johan Obando-Ceron*</em>, Jakob Foerster, Karolina Dziugaite and Pablo Samuel Castro</div>
                  <div class="periodical"> <em> In Reinforcement Learning Conference</em>, 2024 </div>
                  <div class="periodical"> </div>
                  <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a
                      href="https://arxiv.org/pdf/2406.18420" class="btn btn-sm z-depth-0" role="button"
                      rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0"
                      role="button">Bib</a> </div>
                  <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true"
                      data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span
                      class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true"
                      data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div>
                  <div class="abstract hidden">
                    <p>Mixtures of Experts (MoEs) have gained prominence in (self-)supervised learning due to their enhanced 
                      inference efficiency, adaptability to distributed training, and modularity. Previous research has illustrated 
                      that MoEs can significantly boost Deep Reinforcement Learning (DRL) performance by expanding the network's 
                      parameter count while reducing dormant neurons, thereby enhancing the model's learning capacity and ability 
                      to deal with non-stationarity. In this work, we shed more light on MoEs' ability to deal with non-stationarity 
                      and investigate MoEs in DRL settings with "amplified" non-stationarity via multi-task training, providing further 
                      evidence that MoEs improve learning capacity. In contrast to previous work, our multi-task results allow us to 
                      better understand the underlying causes for the beneficial effect of MoE in DRL training, the impact of the various 
                      MoE components, and insights into how best to incorporate them in actor-critic-based DRL networks. Finally, 
                      we also confirm results from previous work.</p>
                  </div>
                  <div class="bibtex hidden">
                    <figure class="highlight">
                      <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">willi2024mixtureexpertsmixturerl</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mixture of Experts in a Mixture of RL settings}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Timon Willi and Johan Obando-Ceron and Jakob Foerster and Karolina Dziugaite and Pablo Samuel Castro}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2406.18420}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre>
                    </figure>
                  </div>
                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#ffb14e"><a
                      href="https://rl-conference.cc" rel="external nofollow noopener" target="_blank">RLC</a></abbr>
                </div>
                <div id="obandoceron2024consistencyhyperparameterselectionvaluebased" class="col-sm-8">
                  <div class="title">On the consistency of hyper-parameter selection in value-based deep reinforcement learning</div>
                  <div class="author">  <em>Johan Obando-Ceron*</em>, João G. M. Araújo*, Aaron Courville and Pablo Samuel Castro
               </div>
                  <div class="periodical"> <em>In Reinforcement Learning Conference</em>, 2024 </div>
                  <div class="periodical"> </div>
                  <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a
                      href="https://arxiv.org/pdf/2406.17523" class="btn btn-sm z-depth-0" role="button"
                      rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0"
                      role="button">Bib</a>
                    </div>
                  <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true"
                      data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span
                      class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true"
                      data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div>
                  <div class="abstract hidden">
                    <p>Deep reinforcement learning (deep RL) has achieved tremendous success on various domains through a combination 
                      of algorithmic design and careful selection of hyper-parameters. Algorithmic improvements are often the result 
                      of iterative enhancements built upon prior approaches, while hyper-parameter choices are typically inherited from 
                      previous methods or fine-tuned specifically for the proposed technique. Despite their crucial impact on performance, 
                      hyper-parameter choices are frequently overshadowed by algorithmic advancements. This paper conducts an extensive 
                      empirical study focusing on the reliability of hyper-parameter selection for value-based deep reinforcement learning 
                      agents, including the introduction of a new score to quantify the consistency and reliability of various hyper-parameters. 
                      Our findings not only help establish which hyper-parameters are most critical to tune, but also help clarify which tunings 
                      remain consistent across different training regimes.</p>
                  </div>
                  <div class="bibtex hidden">
                    <figure class="highlight">
                      <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">obandoceron2024consistencyhyperparameterselectionvaluebased</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the consistency of hyper-parameter selection in value-based deep reinforcement learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Johan Obando-Ceron and João G. M. Araújo and Aaron Courville and Pablo Samuel Castro}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2406.17523}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=seo9V9QRZp}</span><span class="p">,</span>

<span class="p">}</span></code></pre>
                    </figure>
                  </div>
                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#0000bb">
                  <a href="https://icml.cc" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div>
                <div id="ceron2024in" class="col-sm-8">
                  <div class="title">In value-based deep reinforcement learning, a pruned network is a good network</div>
                  <div class="author"> <em>Johan Obando-Ceron</em>, Aaron Courville and Pablo Samuel Castro</div>
                  <div class="periodical"> <em>In Internation Conference on Machine Learning</em>, 2024 </div>
                  <div class="periodical"> </div>
                  <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a
                      href="https://arxiv.org/pdf/2402.12479" class="btn btn-sm z-depth-0" role="button"
                      rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0"
                      role="button">Bib</a> </div>
                  <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true"
                      data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span
                      class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true"
                      data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div>
                  <div class="abstract hidden">
                    <p>Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. 
                      We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning 
                      enables value-based agents to maximize parameter effectiveness. This results in networks that yield dramatic performance 
                      improvements over traditional networks, using only a small fraction of the full network parameters.</p>
                  </div>
                  <div class="bibtex hidden">
                    <figure class="highlight">
                      <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ceron2024in</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{In value-based deep reinforcement learning, a pruned network is a good network  }</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Johan Samir Obando Ceron and Aaron Courville and Pablo Samuel Castro}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-first International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=seo9V9QRZp}</span><span class="p">,</span>
<span class="p">}</span></code></pre>
                    </figure>
                  </div>
                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#0000bb">
                  <a href="https://icml.cc" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div>
                  <div id="ceron2024mixtures" class="col-sm-8">
                  <div class="title">Mixtures of Experts Unlock Parameter Scaling for Deep RL</div>
                  <div class="author"> <em> Johan Obando-Ceron*</em>, Ghada Sokar*, Timon Willi*, 
                    Clare Lyle, Jesse Farebrother, Jakob Nicolaus Foerster, Gintare Karolina Dziugaite,
                    Doina Precup and Pablo Samuel Castro </div>
                  <div class="periodical"> <em>In Internation Conference on Machine Learning</em>, 2024 </div>
                  <div class="periodical"> </div>
                  <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a
                      href="https://openreview.net/pdf?id=X9VMhfFxwn" class="btn btn-sm z-depth-0" role="button"
                      rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0"
                      role="button">Bib</a> </div>
                  <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true"
                      data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span
                      class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true"
                      data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div>
                  <div class="abstract hidden">
                    <p>The recent rapid progress in (self) supervised learning models is in large part predicted by empirical 
                      scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive 
                      for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its 
                      final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in 
                      particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, 
                      evidenced by substantial performance increases across a variety of training regimes and model sizes. 
                      This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.</p>
                  </div>
                  <div class="bibtex hidden">
                    <figure class="highlight">
                      <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ceron2024mixtures</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mixtures of Experts Unlock Parameter Scaling for Deep RL}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Johan Samir Obando Ceron and Ghada Sokar and Timon Willi and Clare Lyle and Jesse Farebrother and Jakob Nicolaus Foerster and Gintare Karolina Dziugaite and Doina Precup and Pablo Samuel Castro},</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-first International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=X9VMhfFxwn}</span><span class="p">,</span>

<span class="p">}</span></code></pre>
                    </figure>
                  </div>
                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#fa8775"><a
                      href="https://cpal.cc" rel="external nofollow noopener" target="_blank">CPAL</a></abbr></div>
                <div id="lee2024jaxpruner" class="col-sm-8">
                  <div class="title">JaxPruner: A concise library for sparsity research</div>
                  <div class="author"> Lee, Joo Hyung, Park, Wonpyo,  Mitchell, Nicole Elyse and  <span class="more-authors"
                      title="click to view 4 more authors"
                      onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == 'more authors' ? 'Pilault, Jonathan and Ceron, Johan Samir Obando and Kim, Han-Byul and Lee, ' : 'more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4
                      more authors</span> </div>
                  <div class="periodical"> <em>In Conference on Parsimony and Learning</em>, 2024 </div>
                  <div class="periodical"> </div>
                  <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a
                      href="https://openreview.net/pdf?id=H2rCZCfXkS" class="btn btn-sm z-depth-0" role="button"
                      rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0"
                      role="button">Bib</a> </div>
                  <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true"
                      data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span
                      class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true"
                      data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div>
                  <div class="abstract hidden">
                    <p>This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. 
                      JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse 
                      training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly 
                      with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate 
                      this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline 
                      experiments on popular benchmarks. Jaxpruner is hosted at github. com/google-research/jaxpruner
                    </p>
                  </div>
                  <div class="bibtex hidden">
                    <figure class="highlight">
                      <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lee2024jaxpruner</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{JaxPruner: A concise library for sparsity research}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lee, Joo Hyung and Park, Wonpyo and Mitchell, Nicole Elyse and Pilault, Jonathan and Ceron, Johan Samir Obando and Kim, Han-Byul and Lee, 
    Namhoon and Frantar, Elias and Long, Yun and Yazdanbakhsh, Amir and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Parsimony and Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre>
                    </figure>
                  </div>
                </div>
              </div>
            </li>
          </ol>
          <h2 class="bibliography">2023</h2>
          <ol class="bibliography">
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#ffd700"><a
                      href="https://neurips.cc" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr>
                </div>
                <div id="ceron2023small" class="col-sm-8">
                  <div class="title">Small batch deep reinforcement learning</div>
                  <div class="author"> <em>Johan Obando-Ceron</em>, Marc G. Bellemare, and Pablo Samuel Castro</div>
                  <div class="periodical"> <em>In Neural Information Processing Systems</em>, 2023 </div>
                  <div class="periodical"> </div>
                  <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a
                      href="https://openreview.net/pdf?id=wPqEvmwFEh" class="btn btn-sm z-depth-0" role="button"
                      rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0"
                      role="button">Bib</a></div>
                  <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true"
                      data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span
                      class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true"
                      data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div>
                  <div class="abstract hidden">
                    <p>In value-based deep reinforcement learning with replay memories, the batch size parameter specifies how 
                      many transitions to sample for each gradient update. Although critical to the learning process, this value 
                      is typically not adjusted when proposing new algorithms. In this work we present a broad empirical study that 
                      suggests reducing the batch size can result in a number of significant performance gains; this is surprising, 
                      as the general tendency when training neural networks is towards larger batch sizes for improved performance. 
                      We complement our experimental findings with a set of empirical analyses towards better understanding this phenomenon..</p>
                  </div>
                  <div class="bibtex hidden">
                    <figure class="highlight">
                      <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ceron2023small</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Small batch deep reinforcement learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ceron, Johan Samir Obando and Bellemare, Marc G and Castro, Pablo Samuel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Thirty-seventh Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre>
                    </figure>
                  </div>
                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#0000bb"><a
                      href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div>
                <div id="schwarzer2023bigger" class="col-sm-8">
                  <div class="title">Bigger, better, faster: Human-level atari with human-level efficiency</div>
                  <div class="author">Max Schwarzer*, <em>Johan Obando-Ceron*</em>, Aaron Courville, Marc G. Bellemare, Rishabh Agarwal* and Pablo Samuel Castro*.</div>
                  <div class="periodical"> <em>In International Conference on Machine Learning</em>, 2023 </div>
                  <div class="periodical"> </div>
                  <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a
                      href="https://proceedings.mlr.press/v202/schwarzer23a/schwarzer23a.pdf" class="btn btn-sm z-depth-0" role="button"
                      rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0"
                      role="button">Bib</a> </div>
                  <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true"
                      data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span
                      class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true"
                      data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div>
                  <div class="abstract hidden">
                    <p>We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the 
                      Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a 
                      number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive 
                      analyses of these design choices and provide insights for future work. We end with a discussion about 
                      updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly 
                      available at https://github. com/google-research/google-research/tree/master/bigger_better_faster.</p>
                  </div>
                  <div class="bibtex hidden">
                    <figure class="highlight">
                      <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">schwarzer2023bigger</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bigger, better, faster: Human-level atari with human-level efficiency}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Schwarzer, Max and Ceron, Johan Samir Obando and Courville, Aaron and Bellemare, Marc G and Agarwal, Rishabh and Castro, Pablo Samuel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{30365--30380}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre>
                    </figure>
                  </div>
                </div>
              </div>
            </li>

          </ol>
          <h2 class="bibliography">2021</h2>
          <ol class="bibliography">
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#0000bb"><a
                      href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div>
                <div id="pmlr-v139-ceron21a" class="col-sm-8">
                  <div class="title">Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research</div>
                  <div class="author"><em>Johan Obando-Ceron</em> and Pablo Samuel Castro.</div>
                  <div class="periodical"> <em>In International Conference on Machine Learning</em>, 2021 </div>
                  <div class="periodical"> </div>
                  <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a
                      href="http://proceedings.mlr.press/v139/ceron21a/ceron21a.pdf" class="btn btn-sm z-depth-0" role="button"
                      rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0"
                      role="button">Bib</a> </div>
                  <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true"
                      data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span
                      class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true"
                      data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div>
                  <div class="abstract hidden">
                    <p>Since the introduction of DQN, a vast majority of reinforcement learning research has focused on reinforcement learning with deep neural 
                      networks as function approximators. New methods are typically evaluated on a set of environments that have now become standard, such as 
                      Atari 2600 games. While these benchmarks help standardize evaluation, their computational cost has the unfortunate side effect of widening 
                      the gap between those with ample access to computational resources, and those without. In this work we argue that, despite the community’s 
                      emphasis on large-scale environments, the traditional small-scale environments can still yield valuable scientific insights and can help reduce 
                      the barriers to entry for underprivileged communities. To substantiate our claims, we empirically revisit the paper which introduced the 
                      Rainbow algorithm [Hessel et al., 2018] and present some new insights into the algorithms used by Rainbow.</p>
                  </div>
                  <div class="bibtex hidden">
                    <figure class="highlight">
                      <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pmlr-v139-ceron21a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ceron, Johan Samir Obando and Castro, Pablo Samuel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 38th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1373--1383}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="s">{18--24 Jul}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>  
<span class="p">}</span></code></pre>
                    </figure>
                  </div>
                </div>
              </div>
            </li>
          </ol>

        </div>
      </article>
    </div>
  </div>
  <footer class="sticky-bottom mt-5">
    <div class="container"> © Copyright 2024 Johan Obando-Ceron. Powered by <a href="https://jekyllrb.com/" target="_blank"
        rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio"
        rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: October 31, 2024. </div>
  </footer>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"
    integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js"
    integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js"
    integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js"
    integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="../assets/js/masonry.js" type="text/javascript"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js"
    integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="../assets/js/zoom.js"></script>
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>
  <script src="../assets/js/no_defer.js"></script>
  <script defer src="../assets/js/common.js"></script>
  <script defer src="../assets/js/copy_code.js" type="text/javascript"></script>
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>
  <script type="text/javascript">window.MathJax = { tex: { tags: "ams" } };</script>
  <script defer type="text/javascript" id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
    type="text/javascript">function progressBarSetup() { "max" in document.createElement("progress") ? (initializeProgressElement(), $(document).on("scroll", function () { progressBar.attr({ value: getCurrentScrollPosition() }) }), $(window).on("resize", initializeProgressElement)) : (resizeProgressBar(), $(document).on("scroll", resizeProgressBar), $(window).on("resize", resizeProgressBar)) } function getCurrentScrollPosition() { return $(window).scrollTop() } function initializeProgressElement() { let e = $("#navbar").outerHeight(!0); $("body").css({ "padding-top": e }), $("progress-container").css({ "padding-top": e }), progressBar.css({ top: e }), progressBar.attr({ max: getDistanceToScroll(), value: getCurrentScrollPosition() }) } function getDistanceToScroll() { return $(document).height() - $(window).height() } function resizeProgressBar() { progressBar.css({ width: getWidthPercentage() + "%" }) } function getWidthPercentage() { return getCurrentScrollPosition() / getDistanceToScroll() * 100 } const progressBar = $("#progress"); window.onload = function () { setTimeout(progressBarSetup, 50) };</script>
</body>

</html>